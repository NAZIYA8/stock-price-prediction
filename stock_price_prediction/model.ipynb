{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "'''\n",
    "@Author: Naziya\n",
    "\n",
    "@Date: 2021-10--05\n",
    "\n",
    "@Last Modified by: Naziya\n",
    "\n",
    "@Last Modified : 2021-10-05\n",
    "\n",
    "@Title : Program Aim is to clean and preprocess the data recieved from kafka and then create a dataframe and build a machine learning model. \n",
    "'''\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.appName('Stock Data processing').getOrCreate()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/10/07 02:45:12 WARN Utils: Your hostname, naziya-Inspiron-N5110 resolves to a loopback address: 127.0.1.1; using 192.168.1.109 instead (on interface wlp9s0)\n",
      "21/10/07 02:45:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/10/07 02:45:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/07 02:45:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.functions import percent_rank\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df = spark.read.csv(\"hdfs://localhost:9000/stock_price_data/stock_data.csv\",header=True)\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Row([\"time\"='[\"2021-10-04 20:00:00\"',  \"open\"=' \"144.12\"',  \"high\"=' \"144.12\"',  \"low\"=' \"144.12\"',  \"close\"=' \"144.12\"',  \"volume\"]=' \"500\"]')"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "preprocessing the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "dataset2=df.withColumnRenamed('[\"time\"','time')\\\n",
    "    .withColumnRenamed(' \"open\"','open')\\\n",
    "    .withColumnRenamed(' \"high\"','high')\\\n",
    "    .withColumnRenamed(' \"low\"','low')\\\n",
    "    .withColumnRenamed(' \"close\"','close')\\\n",
    "    .withColumnRenamed(' \"volume\"]','volume')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "dataset2.describe()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[summary: string, time: string, open: string, high: string, low: string, close: string, volume: string]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "dataset2.show(4)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+---------+---------+---------+---------+--------+\n",
      "|                time|     open|     high|      low|    close|  volume|\n",
      "+--------------------+---------+---------+---------+---------+--------+\n",
      "|[\"2021-10-04 20:0...| \"144.12\"| \"144.12\"| \"144.12\"| \"144.12\"|  \"500\"]|\n",
      "|[\"2021-10-04 19:4...| \"144.27\"| \"144.27\"| \"144.27\"| \"144.27\"|  \"200\"]|\n",
      "|[\"2021-10-04 18:3...| \"144.44\"| \"144.44\"| \"144.44\"| \"144.44\"|  \"101\"]|\n",
      "|[\"2021-10-04 18:2...|  \"144.4\"| \"144.44\"| \"144.35\"| \"144.44\"| \"2895\"]|\n",
      "+--------------------+---------+---------+---------+---------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "new_df = dataset2.withColumn('open', regexp_replace('open', '\"', ''))\\\n",
    "    .withColumn('Time', regexp_replace('time', '\\\\[\"', ''))\\\n",
    "    .withColumn('Time', regexp_replace('time', '\"', ''))\\\n",
    "    .withColumn('High', regexp_replace('high', '\"', ''))\\\n",
    "    .withColumn('Low', regexp_replace('low', '\"', ''))\\\n",
    "    .withColumn('Close', regexp_replace('close', '\"', ''))\\\n",
    "    .withColumn('Volume', regexp_replace('volume', '\"', ''))\\\n",
    "    .withColumn('Volume', regexp_replace('volume', '\\\\]', ''))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Casting the string values to double"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "df2 = new_df\\\n",
    "    .withColumn(\"open\",col(\"Open\").cast(\"double\"))\\\n",
    "    .withColumn(\"high\",col(\"High\").cast(\"double\"))\\\n",
    "    .withColumn(\"low\",col(\"Low\").cast(\"double\"))\\\n",
    "    .withColumn(\"close\",col(\"Close\").cast(\"double\"))\\\n",
    "    .withColumn(\"volume\",col(\"Volume\").cast(\"double\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "df2.show(4)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+------+------+------+------+------+\n",
      "|               Time|  open|  high|   low| close|volume|\n",
      "+-------------------+------+------+------+------+------+\n",
      "|2021-10-04 20:00:00|144.12|144.12|144.12|144.12| 500.0|\n",
      "|2021-10-04 19:45:00|144.27|144.27|144.27|144.27| 200.0|\n",
      "|2021-10-04 18:30:00|144.44|144.44|144.44|144.44| 101.0|\n",
      "|2021-10-04 18:25:00| 144.4|144.44|144.35|144.44|2895.0|\n",
      "+-------------------+------+------+------+------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the vectors from features. Apache MLib takes the input in the form of vectors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "featureassembler=VectorAssembler(inputCols=[\"open\",\"high\",\"low\"],outputCol=\"features\")\n",
    "output=featureassembler.transform(df2)\n",
    "output.select(\"features\").show(5,truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------------+\n",
      "|features              |\n",
      "+----------------------+\n",
      "|[144.12,144.12,144.12]|\n",
      "|[144.27,144.27,144.27]|\n",
      "|[144.44,144.44,144.44]|\n",
      "|[144.4,144.44,144.35] |\n",
      "|[144.35,144.45,144.3] |\n",
      "+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Final data with the features and label(close)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "finalized_data=output.select(\"time\",\"features\",\"close\").sort(\"time\",ascending=True)\n",
    "finalized_data.show(5)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+--------------------+------+\n",
      "|               time|            features| close|\n",
      "+-------------------+--------------------+------+\n",
      "|2021-09-07 11:50:00|[138.4793,138.49,...|138.44|\n",
      "|2021-09-07 11:55:00|[138.45,138.4567,...|138.19|\n",
      "|2021-09-07 12:00:00|[138.18,138.32,13...|138.21|\n",
      "|2021-09-07 12:05:00|[138.21,138.44,13...|138.44|\n",
      "|2021-09-07 12:10:00|[138.46,138.545,1...|138.51|\n",
      "+-------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Splitting data into training and testing data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "final_data = finalized_data.withColumn(\"rank\",percent_rank().over(Window.partitionBy().orderBy(\"time\")))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "train_data = final_data.where(\"rank <= .8\").drop(\"rank\")\n",
    "test_data = final_data.where(\"rank > .8\").drop(\"rank\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "train_data.count()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/10/07 02:57:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1512"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "test_data.count()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/10/07 02:57:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "type(test_data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "test_data.write.parquet('testdata1')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/10/07 02:58:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Building a ML Model (Linear regression)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "linear_reg=LinearRegression(featuresCol='features',labelCol='close')\n",
    "lr_model=linear_reg.fit(train_data)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/10/07 02:59:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/10/07 02:59:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "21/10/07 02:59:29 WARN Instrumentation: [c3b6bd60] regParam is zero, which might cause numerical instability and overfitting.\n",
      "21/10/07 02:59:31 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/10/07 02:59:31 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "21/10/07 02:59:31 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "21/10/07 02:59:31 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "21/10/07 02:59:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Coefficients: [-0.47754351773623854,0.7533940437096085,0.724553223959886]\n",
      "Intercept: -0.06180391786587447\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "predDF = lr_model.transform(test_data)\n",
    "predDF.select(\"features\", \"close\",\"prediction\").show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/10/07 03:00:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+------+------------------+\n",
      "|            features| close|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|[139.76,139.76,13...|139.58|139.61695906025358|\n",
      "|[139.56,139.58,13...|139.33|139.38847299770353|\n",
      "|[139.33,139.46,13...|139.46|139.40579951718823|\n",
      "|[139.46,139.48,13...|139.38|139.39588386582344|\n",
      "|[139.36,139.3865,...|139.33|  139.283858461996|\n",
      "+--------------------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluating the models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "predictionCol=\"prediction\",\n",
    "labelCol=\"close\",\n",
    "metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"RMSE is {rmse:.1f}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/10/07 03:00:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE is 0.1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "lr_model.save(\"hdfs://localhost:9000/stock_price_data_model\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}